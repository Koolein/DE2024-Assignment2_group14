{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b722ba2",
   "metadata": {},
   "source": [
    "### Side note:  Ik heb nu alleen de input en output omgezet naar onze situatie.\n",
    "\n",
    "### Alle transformaties rommel is nu gedaan door onze grootste buddy chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd3cd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(\"spark://spark-master:7077\")\n",
    "sparkConf.setAppName(\"Bitcoin Batch Pipeline\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"2g\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
    "sparkConf.set(\"spark.driver.cores\", \"1\")\n",
    "# create the spark session, which is the entry point to Spark SQL engine.\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "# Load data from BigQuery.\n",
    "df = spark.read \\\n",
    "  .format(\"bigquery\") \\\n",
    "  .load(\"data-engineering-labs-guido.labdataset.bitcoin_data\")    # project_id.datatset.tablename. Use your project id\n",
    "df.printSchema()\n",
    "df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890d6ca6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c666db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verwijder missende waarden\n",
    "cleaned_data = df.na.drop()\n",
    "\n",
    "# Controleer op negatieve prijzen of volumes\n",
    "cleaned_data = cleaned_data.filter((cleaned_data['Open'] >= 0) & \n",
    "                                   (cleaned_data['Price'] >= 0) & \n",
    "                                   (cleaned_data['Vol_'] >= 0))\n",
    "\n",
    "# Toon een voorbeeld van de opgeschoonde data\n",
    "cleaned_data.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc7c8a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ab76b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "# Bereken wekelijkse prijsverandering in procenten\n",
    "transformed_data = cleaned_data.withColumn(\n",
    "    \"Weekly_Change\", ((col(\"Close\") - col(\"Open\")) / col(\"Open\")) * 100\n",
    ")\n",
    "\n",
    "# Bereken volatiliteit\n",
    "transformed_data = transformed_data.withColumn(\n",
    "    \"Volatility\", col(\"High\") - col(\"Low\")\n",
    ")\n",
    "\n",
    "# Toon de getransformeerde data\n",
    "transformed_data.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229f91cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74edd06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import month, year, avg, sum, max\n",
    "\n",
    "# Voeg maand- en jaarkolommen toe\n",
    "transformed_data = transformed_data.withColumn(\"Month\", month(col(\"Date\"))) \\\n",
    "                                   .withColumn(\"Year\", year(col(\"Date\")))\n",
    "\n",
    "# Bereken maandelijkse aggregaties\n",
    "monthly_data = transformed_data.groupBy(\"Year\", \"Month\") \\\n",
    "    .agg(\n",
    "        avg(\"Close\").alias(\"Avg_Close\"),\n",
    "        sum(\"Volume\").alias(\"Total_Volume\"),\n",
    "        max(\"Volatility\").alias(\"Max_Volatility\")\n",
    "    )\n",
    "\n",
    "# Toon maandelijkse resultaten\n",
    "monthly_data.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84def9ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4960c42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd7e0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Cloud Storage bucket for temporary BigQuery export data used by the connector.\n",
    "bucket = \"de_jads_tempguido\"  # use your bucket \n",
    "spark.conf.set('temporaryGcsBucket', bucket)\n",
    "# Setup hadoop fs configuration for schema gs://\n",
    "conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "conf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "# Saving the data to BigQuery\n",
    "ordered_word_count.write.format('bigquery') \\\n",
    "  .option('table', 'data-engineering-labs-guido.labdataset.btc_version1') \\\n",
    "  .mode(\"append\") \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eeb0f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the spark context\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
